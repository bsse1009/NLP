{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "c3_w2_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZJZZojxKqfTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27718a13-774b-489e-f8ff-420318f9c8f2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install trax  "
      ],
      "metadata": {
        "id": "A9RCxQ3yq8qA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RVSwzQ5Bwt0m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "import pickle\n",
        "import numpy\n",
        "import random as rnd\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "\n",
        "# set random seed\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5sDs36m81g6f"
      },
      "outputs": [],
      "source": [
        "dirname = '/content/drive/MyDrive/Week 2/data/'\n",
        "lines = [] # storing all the lines in a variable. \n",
        "for filename in os.listdir(dirname):\n",
        "    with open(os.path.join(dirname, filename), encoding='utf-8') as files:\n",
        "        for line in files:\n",
        "            # remove leading and trailing whitespace\n",
        "            pure_line = line.strip()\n",
        "            \n",
        "            # if pure_line is not the empty string,\n",
        "            if pure_line:\n",
        "                # append it to the list\n",
        "                lines.append(pure_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-zMCe7aJkGwA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c41871-fb1d-4ab3-d7c2-8ccc80faf9f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 THE TWO GENTLEMEN OF VERONA\n",
            "Sample line at position 999 If I lose them, thus find I by their loss\n"
          ]
        }
      ],
      "source": [
        "n_lines = len(lines)\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UBO9jI8EkGwE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb160678-9493-4413-cc2b-05e87fa26709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 the two gentlemen of verona\n",
            "Sample line at position 999 if i lose them, thus find i by their loss\n"
          ]
        }
      ],
      "source": [
        "# go through each line\n",
        "for i, line in enumerate(lines):\n",
        "    # convert to all lowercase\n",
        "    lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UdXZAK4rqafe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38010071-ae9c-4da1-f136-b4471a1beb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines for training: 124097\n",
            "Number of lines for validation: 1000\n"
          ]
        }
      ],
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IO4NSPkOITNK"
      },
      "outputs": [],
      "source": [
        "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: line_to_tensor\n",
        "def line_to_tensor(line, EOS_int=1):\n",
        "    \"\"\"Turns a line of text into a tensor\n",
        "\n",
        "    Args:\n",
        "        line (str): A single line of text.\n",
        "        EOS_int (int, optional): End-of-sentence integer. Defaults to 1.\n",
        "\n",
        "    Returns:\n",
        "        list: a list of integers (unicode values) for the characters in the `line`.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize the tensor as an empty list\n",
        "    tensor = []\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    # for each character:\n",
        "    for c in line:\n",
        "        \n",
        "        # convert to unicode int\n",
        "        c_int = ord(c)\n",
        "        \n",
        "        # append the unicode integer to the tensor list\n",
        "        tensor.append(c_int)\n",
        "    \n",
        "    # include the end-of-sentence integer\n",
        "    tensor.append(EOS_int)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "D9Z_vtI7tTcw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6859a0d1-2fd9-49d6-c1aa-34e3d884784e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 98, 99, 32, 120, 121, 122, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Testing your output\n",
        "line_to_tensor('abc xyz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OMingz5xzrGD"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: data_generator\n",
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "                Keep in mind that the length of the tensor is always 1 + the length\n",
        "                of the original line of characters.\n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "\n",
        "    Yields:\n",
        "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
        "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater than or equal to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index) \n",
        "                            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index +=1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                # so that the tensor plus padding will have length `max_length`\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for this tensor_pad is 1 whereever tensor_pad is not\n",
        "                # 0 and 0 whereever tensor_pad is 0, i.e. if tensor_pad is\n",
        "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
        "                # [1, 1, 1, 0, 0, 0]\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask) # @ KEEPTHIS\n",
        "               \n",
        "            # convert the batch (data type list) to a numpy array\n",
        "            batch_np_arr = np.array(batch)\n",
        "            mask_np_arr = np.array(mask)\n",
        "            \n",
        "            ### END CODE HERE ##\n",
        "            \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XJ1HBoEHwt0p",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4004329c-4096-48e9-9213-477f8b6ccc7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[49, 50, 51, 52, 53, 54, 55, 56, 57,  1],\n",
              "              [50, 51, 52, 53, 54, 55, 56, 57, 48,  1]], dtype=int32),\n",
              " DeviceArray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "              [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Try out your data generator\n",
        "tmp_lines = ['12345678901', #length 11\n",
        "             '123456789', # length 9\n",
        "             '234567890', # length 9\n",
        "             '345678901'] # length 9\n",
        "\n",
        "# Get a batch size of 2, max length 10\n",
        "tmp_data_gen = data_generator(batch_size=2, \n",
        "                              max_length=10, \n",
        "                              data_lines=tmp_lines,\n",
        "                              shuffle=False)\n",
        "\n",
        "# get one batch\n",
        "tmp_batch = next(tmp_data_gen)\n",
        "\n",
        "# view the batch\n",
        "tmp_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hww76f8_wt0x"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: GRULM\n",
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    \"\"\"Returns a GRU language model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int, optional): Size of the vocabulary. Defaults to 256.\n",
        "        d_model (int, optional): Depth of embedding (n_units in the GRU cell). Defaults to 512.\n",
        "        n_layers (int, optional): Number of GRU layers. Defaults to 2.\n",
        "        mode (str, optional): 'train', 'eval' or 'predict', predict mode is for fast inference. Defaults to \"train\".\n",
        "\n",
        "    Returns:\n",
        "        trax.layers.combinators.Serial: A GRU language model as a layer that maps from a tensor of tokens to activations over a vocab set.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "    model = tl.Serial( \n",
        "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax(), # Log Softmax\n",
        "    ) \n",
        "    ### END CODE HERE ###\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kvQ_jf52-JAn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc86b6e0-bd47-4362-9da3-81d726937fe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_256_512\n",
            "  GRU_512\n",
            "  GRU_512\n",
            "  Dense_256\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "# testing your model\n",
        "model = GRULM()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5Birerv82mLv"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "_kbtfz4T_m7x"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from trax.supervised import training\n",
        "\n",
        "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: train_model\n",
        "def train_model(model, data_generator, lines, eval_lines, batch_size=32, max_length=64, n_steps=500, output_dir='/content/drive/MyDrive/Week 2/model/'): \n",
        "    \"\"\"Function that trains the model\n",
        "\n",
        "    Args:\n",
        "        model (trax.layers.combinators.Serial): GRU model.\n",
        "        data_generator (function): Data generator function.\n",
        "        batch_size (int, optional): Number of lines per batch. Defaults to 32.\n",
        "        max_length (int, optional): Maximum length allowed for a line to be processed. Defaults to 64.\n",
        "        lines (list): List of lines to use for training. Defaults to lines.\n",
        "        eval_lines (list): List of lines to use for evaluation. Defaults to eval_lines.\n",
        "        n_steps (int, optional): Number of steps to train. Defaults to 1.\n",
        "        output_dir (str, optional): Relative path of directory to save model. Defaults to \"model/\".\n",
        "\n",
        "    Returns:\n",
        "        trax.supervised.training.Loop: Training loop for the model.\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
        "    infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "    \n",
        "    bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
        "    infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "    \n",
        "    train_task = training.TrainTask( \n",
        "        labeled_data=infinite_train_generator, # Use infinite train data generator\n",
        "        loss_layer=tl.CrossEntropyLoss(),   # Don't forget to instantiate this object\n",
        "        optimizer=trax.optimizers.Adam(0.0005)      # Don't forget to add the learning rate parameter TO 0.0005\n",
        "    ) \n",
        "    \n",
        "    eval_task = training.EvalTask( \n",
        "        labeled_data=infinite_eval_generator,    # Use infinite eval data generator\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()], # Don't forget to instantiate these objects\n",
        "        n_eval_batches=3  # For better evaluation accuracy in reasonable time \n",
        "    ) \n",
        "    \n",
        "    training_loop = training.Loop(model, \n",
        "                                  train_task, \n",
        "                                  eval_tasks=[eval_task], \n",
        "                                  output_dir=output_dir) \n",
        "\n",
        "    training_loop.run(n_steps=n_steps)\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # We return this because it contains a handle to the model, which has the weights etc.\n",
        "    return training_loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "SwP646GpK3pD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5117abe-824a-4e37-be3e-12b8e046cea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/lib/xla_bridge.py:413: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  \"jax.host_count has been renamed to jax.process_count. This alias \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 3411200\n",
            "Step      1: Ran 1 train steps in 9.72 secs\n",
            "Step      1: train CrossEntropyLoss |  5.54510975\n",
            "Step      1: eval  CrossEntropyLoss |  5.54101912\n",
            "Step      1: eval          Accuracy |  0.15085300\n",
            "\n",
            "Step    100: Ran 99 train steps in 194.79 secs\n",
            "Step    100: train CrossEntropyLoss |  3.40271235\n",
            "Step    100: eval  CrossEntropyLoss |  2.98650829\n",
            "Step    100: eval          Accuracy |  0.16061198\n",
            "\n",
            "Step    200: Ran 100 train steps in 192.26 secs\n",
            "Step    200: train CrossEntropyLoss |  2.76606226\n",
            "Step    200: eval  CrossEntropyLoss |  2.55814902\n",
            "Step    200: eval          Accuracy |  0.25560746\n",
            "\n",
            "Step    300: Ran 100 train steps in 193.78 secs\n",
            "Step    300: train CrossEntropyLoss |  2.44923663\n",
            "Step    300: eval  CrossEntropyLoss |  2.35520299\n",
            "Step    300: eval          Accuracy |  0.30742135\n",
            "\n",
            "Step    400: Ran 100 train steps in 186.88 secs\n",
            "Step    400: train CrossEntropyLoss |  2.29153299\n",
            "Step    400: eval  CrossEntropyLoss |  2.20691681\n",
            "Step    400: eval          Accuracy |  0.36240835\n",
            "\n",
            "Step    500: Ran 100 train steps in 186.83 secs\n",
            "Step    500: train CrossEntropyLoss |  2.20070577\n",
            "Step    500: eval  CrossEntropyLoss |  2.17656406\n",
            "Step    500: eval          Accuracy |  0.35527386\n"
          ]
        }
      ],
      "source": [
        "# Train the model 1 step and keep the `trax.supervised.training.Loop` object.\n",
        "output_dir = '/content/drive/MyDrive/Week 2/model/'\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(output_dir)\n",
        "except OSError as e:\n",
        "    pass\n",
        "\n",
        "training_loop = train_model(GRULM(), data_generator, lines=lines, eval_lines=eval_lines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "3OtmlEuOwt1D"
      },
      "outputs": [],
      "source": [
        "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: test_model\n",
        "def test_model(preds, target):\n",
        "    \"\"\"Function to test the model.\n",
        "\n",
        "    Args:\n",
        "        preds (jax.interpreters.xla.DeviceArray): Predictions of a list of batches of tensors corresponding to lines of text.\n",
        "        target (jax.interpreters.xla.DeviceArray): Actual list of batches of tensors corresponding to lines of text.\n",
        "\n",
        "    Returns:\n",
        "        float: log_perplexity of the model.\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ###\n",
        "\n",
        "    log_p = np.sum(preds * tl.one_hot(target, preds.shape[-1]), axis= -1) # HINT: tl.one_hot() should replace one of the Nones\n",
        "\n",
        "    non_pad = 1.0 - np.equal(target, 0)          # You should check if the target equals 0\n",
        "    log_p = log_p * non_pad                             # Get rid of the padding    \n",
        "    \n",
        "    log_ppx = np.sum(log_p) / np.sum(non_pad) # Remember to set the axis properly when summing up\n",
        "    log_ppx = np.mean(log_ppx) # Compute the mean of the previous expression\n",
        "    \n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return -log_ppx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "xl8X0FPAwt1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c37aae3f-30c1-42d2-a631-62a4c2dddb3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The log perplexity and perplexity of your model are respectively 2.391001 10.924423\n"
          ]
        }
      ],
      "source": [
        "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# Testing \n",
        "model = GRULM()\n",
        "model.init_from_file('/content/drive/MyDrive/Week 2/model/model.pkl.gz')\n",
        "batch = next(data_generator(batch_size, max_length, lines, shuffle=False))\n",
        "preds = model(batch[0])\n",
        "log_ppx = test_model(preds, batch[1])\n",
        "print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xrOJHbXewt1J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f81c3b-2585-4e03-942e-903ea2e1b262"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s tolchen avy on nou wruule,\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to generate some news sentence\n",
        "def gumbel_sample(log_probs, temperature=1.0):\n",
        "    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\n",
        "    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probs.shape)\n",
        "    g = -np.log(-np.log(u))\n",
        "    return np.argmax(log_probs + g * temperature, axis=-1)\n",
        "\n",
        "def predict(num_chars, prefix):\n",
        "    inp = [ord(c) for c in prefix]\n",
        "    result = [c for c in prefix]\n",
        "    max_len = len(prefix) + num_chars\n",
        "    for _ in range(num_chars):\n",
        "        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\n",
        "        outp = model(cur_inp[None, :])  # Add batch dim.\n",
        "        next_char = gumbel_sample(outp[0, len(inp)])\n",
        "        inp += [int(next_char)]\n",
        "       \n",
        "        if inp[-1] == 1:\n",
        "            break  # EOS\n",
        "        result.append(chr(int(next_char)))\n",
        "    \n",
        "    return \"\".join(result)\n",
        "\n",
        "print(predict(32, \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Raojyhw3z7HE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "339e68ae-6da5-4634-fed9-044f5b37bfed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be sill so nean, i lmittedsinok,\n",
            "the love-chingy\tand on the cill,\n",
            "(thiwh hir,\n"
          ]
        }
      ],
      "source": [
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n",
        "print(predict(32, \"\"))\n"
      ]
    }
  ]
}